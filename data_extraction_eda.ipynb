{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0480057a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc02a59a",
   "metadata": {},
   "source": [
    "# Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "10c9d64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "UCI_ZIP_URL = \"https://archive.ics.uci.edu/static/public/502/online+retail+ii.zip\"\n",
    "DATA_DIR = Path(\"data/raw\")\n",
    "ZIP_PATH = DATA_DIR / \"online_retail_ii.zip\"\n",
    "XLSX_NAME = \"online_retail_II.xlsx\"\n",
    "XLSX_PATH = DATA_DIR / XLSX_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ad8da8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url: str, out_path: Path, chunk_size: int = 1024 * 1024) -> None:\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if out_path.exists() and out_path.stat().st_size > 0:\n",
    "        print(f\"[skip] {out_path} already exists ({out_path.stat().st_size} bytes)\")\n",
    "        return\n",
    "\n",
    "    print(f\"[download] {url}\")\n",
    "    with requests.get(url, stream=True, timeout=60) as r:\n",
    "        r.raise_for_status()\n",
    "        total = int(r.headers.get(\"Content-Length\", 0))\n",
    "\n",
    "        tmp_path = out_path.with_suffix(out_path.suffix + \".part\")\n",
    "        downloaded = 0\n",
    "        with open(tmp_path, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    downloaded += len(chunk)\n",
    "                    if total:\n",
    "                        pct = 100 * downloaded / total\n",
    "                        print(f\"\\r  {downloaded:,}/{total:,} bytes ({pct:5.1f}%)\", end=\"\")\n",
    "        if total:\n",
    "            print()\n",
    "        tmp_path.replace(out_path)\n",
    "    print(f\"[saved] {out_path}\")\n",
    "\n",
    "\n",
    "def unzip(zip_path: Path, extract_dir: Path) -> None:\n",
    "    extract_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if (extract_dir / XLSX_NAME).exists():\n",
    "        print(f\"[skip] already extracted: {(extract_dir / XLSX_NAME)}\")\n",
    "        return\n",
    "\n",
    "    print(f\"[unzip] {zip_path} -> {extract_dir}\")\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
    "        zf.extractall(extract_dir)\n",
    "    print(\"[done] extraction complete\")\n",
    "\n",
    "\n",
    "def load_excel(xlsx_path: Path) -> dict[str, pd.DataFrame]:\n",
    "    # The Excel has two sheets: \"Year 2009-2010\" and \"Year 2010-2011\"\n",
    "    print(f\"[read] {xlsx_path}\")\n",
    "    sheets = pd.read_excel(xlsx_path, sheet_name=None, engine=\"openpyxl\")\n",
    "    print(\"[sheets]\", list(sheets.keys()))\n",
    "    for k, df in sheets.items():\n",
    "        print(f\"  - {k}: {df.shape[0]:,} rows x {df.shape[1]} cols\")\n",
    "    return sheets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8cd8358a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[download] https://archive.ics.uci.edu/static/public/502/online+retail+ii.zip\n",
      "[saved] data/raw/online_retail_ii.zip\n",
      "[unzip] data/raw/online_retail_ii.zip -> data/raw\n",
      "[done] extraction complete\n",
      "[read] data/raw/online_retail_II.xlsx\n",
      "[sheets] ['Year 2009-2010', 'Year 2010-2011']\n",
      "  - Year 2009-2010: 525,461 rows x 8 cols\n",
      "  - Year 2010-2011: 541,910 rows x 8 cols\n"
     ]
    }
   ],
   "source": [
    "download_file(UCI_ZIP_URL, ZIP_PATH)\n",
    "unzip(ZIP_PATH, DATA_DIR)\n",
    "\n",
    "if not XLSX_PATH.exists():\n",
    "    # Some zips may extract into a nested folder; search for the xlsx if needed\n",
    "    candidates = list(DATA_DIR.rglob(\"*.xlsx\"))\n",
    "    raise FileNotFoundError(f\"Could not find {XLSX_NAME}. Found: {[str(c) for c in candidates]}\")\n",
    "\n",
    "sheets = load_excel(XLSX_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a26ecac",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1466f97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First three rows: \n",
      "   Invoice StockCode                          Description  Quantity  \\\n",
      "0  489434     85048  15CM CHRISTMAS GLASS BALL 20 LIGHTS        12   \n",
      "1  489434    79323P                   PINK CHERRY LIGHTS        12   \n",
      "2  489434    79323W                  WHITE CHERRY LIGHTS        12   \n",
      "\n",
      "          InvoiceDate  Price  Customer ID         Country  \n",
      "0 2009-12-01 07:45:00   6.95      13085.0  United Kingdom  \n",
      "1 2009-12-01 07:45:00   6.75      13085.0  United Kingdom  \n",
      "2 2009-12-01 07:45:00   6.75      13085.0  United Kingdom  \n",
      "All records number:  (1067371, 8)\n",
      "Column name:  ['Invoice', 'StockCode', 'Description', 'Quantity', 'InvoiceDate', 'Price', 'Customer ID', 'Country']\n"
     ]
    }
   ],
   "source": [
    "# Data Overview\n",
    "df_all = pd.concat(sheets.values(), ignore_index=True)\n",
    "column = df_all\n",
    "print(\"First three rows: \\n\", df_all.head(3))\n",
    "print(\"All records number: \", df_all.shape)\n",
    "print(\"Column name: \", df_all.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3c498fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null counts per column: \n",
      " Customer ID    243007\n",
      "Description      4382\n",
      "Invoice             0\n",
      "StockCode           0\n",
      "Quantity            0\n",
      "InvoiceDate         0\n",
      "Price               0\n",
      "Country             0\n",
      "dtype: int64\n",
      "\n",
      " Unique products: 5,305\n",
      "Unique customers: 5,942\n"
     ]
    }
   ],
   "source": [
    "# Null value exploration\n",
    "print(\"Null counts per column: \\n\", df_all.isna().sum().sort_values(ascending=False))\n",
    "\n",
    "# Uniqueness\n",
    "n_products = df_all[\"StockCode\"].nunique(dropna=True)\n",
    "n_customers = df_all[\"Customer ID\"].nunique(dropna=True)\n",
    "print(f\"\\n Unique products: {n_products:,}\")\n",
    "print(f\"Unique customers: {n_customers:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "da3234c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique countries: 43\n",
      "United Kingdom rows: 981,330 (91.94%)\n",
      "\n",
      " InvoiceDate range:\n",
      "Min InvoiceDate: 2009-12-01 07:45:00\n",
      "Max InvoiceDate: 2011-12-09 12:50:00\n",
      "Invalid/NaT InvoiceDate: 0\n"
     ]
    }
   ],
   "source": [
    "# Country-level exploration\n",
    "n_countries = df_all[\"Country\"].nunique(dropna=True)\n",
    "uk_count = (df_all[\"Country\"] == \"United Kingdom\").sum()\n",
    "uk_pct = uk_count / len(df_all) * 100\n",
    "print(f\"Unique countries: {n_countries:,}\")\n",
    "print(f\"United Kingdom rows: {uk_count:,} ({uk_pct:.2f}%)\")\n",
    "\n",
    "# Range of InvoiceDate\n",
    "print(\"\\n InvoiceDate range:\")\n",
    "invoice_dt = pd.to_datetime(df_all[\"InvoiceDate\"], errors=\"coerce\")\n",
    "print(f\"Min InvoiceDate: {invoice_dt.min()}\")\n",
    "print(f\"Max InvoiceDate: {invoice_dt.max()}\")\n",
    "print(f\"Invalid/NaT InvoiceDate: {invoice_dt.isna().sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8c301894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact-duplicate rows (all columns): 67242\n",
      "\n",
      "Top duplicate groups (all columns):\n",
      "Invoice  StockCode  Description                          Quantity  InvoiceDate          Price  Customer ID  Country       \n",
      "555524   22698      PINK REGENCY TEACUP AND SAUCER       1         2011-06-05 11:37:00  2.95   16923.0      United Kingdom    20\n",
      "         22697      GREEN REGENCY TEACUP AND SAUCER      1         2011-06-05 11:37:00  2.95   16923.0      United Kingdom    12\n",
      "537224   70007      HI TEC ALPINE HAND WARMER            1         2010-12-05 16:24:00  1.65   13174.0      United Kingdom    10\n",
      "572861   22775      PURPLE DRAWERKNOB ACRYLIC EDWARDIAN  12        2011-10-26 12:46:00  1.25   14102.0      United Kingdom     8\n",
      "536749   21415      CLAM SHELL SMALL                     2         2010-12-02 13:49:00  2.10   17976.0      United Kingdom     6\n",
      "536874   22866      HAND WARMER SCOTTY DOG DESIGN        1         2010-12-03 11:35:00  2.10   16891.0      United Kingdom     6\n",
      "537042   21579      LOLITA  DESIGN  COTTON TOTE BAG      1         2010-12-05 10:45:00  2.25   13838.0      United Kingdom     6\n",
      "537196   21811      CHRISTMAS HANGING HEART WITH BELL    1         2010-12-05 13:55:00  1.25   15426.0      United Kingdom     6\n",
      "537265   79000      MOROCCAN TEA GLASS                   12        2010-12-06 11:26:00  0.85   15919.0      United Kingdom     6\n",
      "536749   22174      PHOTO CUBE                           2         2010-12-02 13:49:00  1.65   17976.0      United Kingdom     6\n",
      "dtype: int64\n",
      "\n",
      "Example of an exact-duplicate group:\n",
      "Invoice StockCode                    Description  Quantity         InvoiceDate  Price  Customer ID        Country\n",
      " 555524     22698 PINK REGENCY TEACUP AND SAUCER         1 2011-06-05 11:37:00   2.95      16923.0 United Kingdom\n",
      " 555524     22698 PINK REGENCY TEACUP AND SAUCER         1 2011-06-05 11:37:00   2.95      16923.0 United Kingdom\n",
      " 555524     22698 PINK REGENCY TEACUP AND SAUCER         1 2011-06-05 11:37:00   2.95      16923.0 United Kingdom\n",
      " 555524     22698 PINK REGENCY TEACUP AND SAUCER         1 2011-06-05 11:37:00   2.95      16923.0 United Kingdom\n",
      " 555524     22698 PINK REGENCY TEACUP AND SAUCER         1 2011-06-05 11:37:00   2.95      16923.0 United Kingdom\n",
      " 555524     22698 PINK REGENCY TEACUP AND SAUCER         1 2011-06-05 11:37:00   2.95      16923.0 United Kingdom\n",
      " 555524     22698 PINK REGENCY TEACUP AND SAUCER         1 2011-06-05 11:37:00   2.95      16923.0 United Kingdom\n",
      " 555524     22698 PINK REGENCY TEACUP AND SAUCER         1 2011-06-05 11:37:00   2.95      16923.0 United Kingdom\n",
      " 555524     22698 PINK REGENCY TEACUP AND SAUCER         1 2011-06-05 11:37:00   2.95      16923.0 United Kingdom\n",
      " 555524     22698 PINK REGENCY TEACUP AND SAUCER         1 2011-06-05 11:37:00   2.95      16923.0 United Kingdom\n",
      " 555524     22698 PINK REGENCY TEACUP AND SAUCER         1 2011-06-05 11:37:00   2.95      16923.0 United Kingdom\n",
      " 555524     22698 PINK REGENCY TEACUP AND SAUCER         1 2011-06-05 11:37:00   2.95      16923.0 United Kingdom\n",
      " 555524     22698 PINK REGENCY TEACUP AND SAUCER         1 2011-06-05 11:37:00   2.95      16923.0 United Kingdom\n",
      " 555524     22698 PINK REGENCY TEACUP AND SAUCER         1 2011-06-05 11:37:00   2.95      16923.0 United Kingdom\n",
      " 555524     22698 PINK REGENCY TEACUP AND SAUCER         1 2011-06-05 11:37:00   2.95      16923.0 United Kingdom\n",
      " 555524     22698 PINK REGENCY TEACUP AND SAUCER         1 2011-06-05 11:37:00   2.95      16923.0 United Kingdom\n",
      " 555524     22698 PINK REGENCY TEACUP AND SAUCER         1 2011-06-05 11:37:00   2.95      16923.0 United Kingdom\n",
      " 555524     22698 PINK REGENCY TEACUP AND SAUCER         1 2011-06-05 11:37:00   2.95      16923.0 United Kingdom\n",
      " 555524     22698 PINK REGENCY TEACUP AND SAUCER         1 2011-06-05 11:37:00   2.95      16923.0 United Kingdom\n",
      " 555524     22698 PINK REGENCY TEACUP AND SAUCER         1 2011-06-05 11:37:00   2.95      16923.0 United Kingdom\n"
     ]
    }
   ],
   "source": [
    "# Duplication Check\n",
    "dup_mask_all = df_all.duplicated(keep=False)\n",
    "print(\"Exact-duplicate rows (all columns):\", dup_mask_all.sum())\n",
    "\n",
    "if dup_mask_all.any():\n",
    "    dup_all = df_all[dup_mask_all].copy()\n",
    "\n",
    "    grp_sizes = dup_all.groupby(list(df_all.columns), dropna=False).size().sort_values(ascending=False)\n",
    "    print(\"\\nTop duplicate groups (all columns):\")\n",
    "    print(grp_sizes.head(10))\n",
    "\n",
    "    first_key = grp_sizes.index[0]\n",
    "\n",
    "    key_df = pd.DataFrame([first_key], columns=df_all.columns)\n",
    "\n",
    "    first_group = dup_all.merge(key_df, on=list(df_all.columns), how=\"inner\")\n",
    "    print(\"\\nExample of an exact-duplicate group:\")\n",
    "    print(first_group.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "925511bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Among duplicated rows, unique InvoiceDate counts per Invoice (top 20):\n",
      "Invoice\n",
      "519730    2\n",
      "536591    2\n",
      "567183    2\n",
      "543812    1\n",
      "544068    1\n",
      "544040    1\n",
      "543973    1\n",
      "543907    1\n",
      "543899    1\n",
      "543816    1\n",
      "543815    1\n",
      "543814    1\n",
      "489517    1\n",
      "544074    1\n",
      "543804    1\n",
      "543803    1\n",
      "543802    1\n",
      "543800    1\n",
      "543731    1\n",
      "543653    1\n",
      "Name: InvoiceDate, dtype: int64\n",
      "\n",
      "% of duplicated invoices where all duplicated lines share the exact same InvoiceDate: 99.94%\n"
     ]
    }
   ],
   "source": [
    "# Duplication Check\n",
    "dup_all = df_all[dup_mask_all].copy()\n",
    "\n",
    "if len(dup_all) > 0:\n",
    "    # 2) For each duplicated invoice, count how many unique InvoiceDate values it has\n",
    "    #    If the duplicates are “split/recording artifacts”, many invoices will have exactly 1 unique timestamp.\n",
    "    inv_dt_nunique = (\n",
    "        dup_all.groupby(\"Invoice\")[\"InvoiceDate\"]\n",
    "        .nunique(dropna=False)\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "\n",
    "    print(\"\\nAmong duplicated rows, unique InvoiceDate counts per Invoice (top 20):\")\n",
    "    print(inv_dt_nunique.head(20))\n",
    "\n",
    "    share_same_ts = (inv_dt_nunique == 1).mean() * 100\n",
    "    print(f\"\\n% of duplicated invoices where all duplicated lines share the exact same InvoiceDate: {share_same_ts:.2f}%\")\n",
    "\n",
    "    # 3) Show an example invoice whose duplicated lines all share the same InvoiceDate\n",
    "    example_invoice = inv_dt_nunique[inv_dt_nunique == 1].index[0]\n",
    "    ex = dup_all[dup_all[\"Invoice\"] == example_invoice].sort_values(\n",
    "        [\"InvoiceDate\", \"StockCode\", \"Description\", \"Quantity\", \"Price\", \"Customer ID\", \"Country\"],\n",
    "        kind=\"mergesort\"\n",
    "    )\n",
    "else:\n",
    "    print(\"No exact-duplicate rows found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
